====================================================================================================
EMOTION CLASSIFICATION MODEL COMPARISON BENCHMARK - CAPSTONE PROJECT
====================================================================================================
Date: 2025-10-12 19:45:46
Test Dataset: Emotion (HuggingFace) - Filtered to 7 emotion classes
Target Classes: anger, disgust, fear, joy, neutral, sadness, surprise
Source: https://huggingface.co/datasets/emotion
Device: CPU

MODELS EVALUATED:
----------------------------------------------------------------------------------------------------
1. Custom XLM-RoBERTa (Emoticoach) (~278M)
   C:\Users\John Carlo\emoticoach\emoticoach\Backend\AIModel

2. RoBERTa-Large (~355M)
   j-hartmann/emotion-english-roberta-large

3. DistilRoBERTa-Base (~82M)
   j-hartmann/emotion-english-distilroberta-base

4. DeBERTa-v3-Large (~434M)
   Tanneru/Emotion-Classification-DeBERTa-v3-Large


OVERALL PERFORMANCE COMPARISON:
----------------------------------------------------------------------------------------------------
                          Model       Architecture Parameters Accuracy F1-Macro F1-Weighted Avg Confidence Error Rate
Custom XLM-RoBERTa (Emoticoach)   XLM-RoBERTa-Base      ~278M   87.80%   64.04%      84.48%         97.29%     12.20%
                  RoBERTa-Large      RoBERTa-Large      ~355M   83.70%   59.51%      80.42%         95.38%     16.30%
             DistilRoBERTa-Base DistilRoBERTa-Base       ~82M   83.90%   52.39%      80.62%         95.21%     16.10%
               DeBERTa-v3-Large   DeBERTa-v3-Large      ~434M   91.05%   79.61%      87.49%         98.88%      8.95%


INFERENCE SPEED COMPARISON:
----------------------------------------------------------------------------------------------------
                          Model Load Time Total Inference Avg per Sample Samples/Second
Custom XLM-RoBERTa (Emoticoach)     4.23s         119.35s        59.67ms          16.76
                  RoBERTa-Large     2.93s         433.02s       216.51ms           4.62
             DistilRoBERTa-Base     1.64s          57.49s        28.74ms          34.79
               DeBERTa-v3-Large     2.66s        1081.47s       540.74ms           1.85


DETAILED METRICS COMPARISON:
----------------------------------------------------------------------------------------------------
                          Model Precision (Macro) Recall (Macro) Precision (Weighted) Recall (Weighted)  Correct  Incorrect
Custom XLM-RoBERTa (Emoticoach)            61.27%         68.00%               81.97%            87.80%     1756        244
                  RoBERTa-Large            56.44%         63.37%               77.58%            83.70%     1674        326
             DistilRoBERTa-Base            49.94%         55.38%               77.74%            83.90%     1678        322
               DeBERTa-v3-Large            77.97%         81.57%               84.58%            91.05%     1821        179


KEY FINDINGS:
----------------------------------------------------------------------------------------------------
• Best Overall Performance: DeBERTa-v3-Large (Accuracy: 91.05%)
• Best F1-Score: DeBERTa-v3-Large (F1-Weighted: 87.49%)
• Fastest Model: DistilRoBERTa-Base (28.74ms per sample)
